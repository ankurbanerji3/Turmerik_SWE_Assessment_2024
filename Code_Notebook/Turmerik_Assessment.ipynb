{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Necessary Libraries"
      ],
      "metadata": {
        "id": "qcNt8LaxWNtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1"
      ],
      "metadata": {
        "id": "AGCVzpKuJFoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G34wqZY3bSUJ",
        "outputId": "204ad8dd-2c47-440a-e5e2-0602fa3a504a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.51.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.51.0-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.5/383.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.5.0 openai-1.51.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating queries.json"
      ],
      "metadata": {
        "id": "wY4YsRQ9wdMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2"
      ],
      "metadata": {
        "id": "FKBL8eh1JC15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import xml.etree.ElementTree as ET\n",
        "import json\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key='Your API Keys',\n",
        ")\n",
        "\n",
        "# Function to parse the XML file and extract relevant patient information\n",
        "def extract_patient_info(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract patient ID (using your XML structure as a guide)\n",
        "    namespace = {'hl7': 'urn:hl7-org:v3'}\n",
        "    patient_id = root.find('.//hl7:id', namespace).attrib['extension']\n",
        "\n",
        "    # Extract patient details to summarize\n",
        "    patient_info = ET.tostring(root, encoding='unicode')\n",
        "\n",
        "    return patient_id, patient_info\n",
        "\n",
        "# Function to summarize patient information using OpenAI API\n",
        "def summarize_patient_info(patient_info):\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",  # or \"gpt-4\" if you have access\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a concise medical assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Summarize the following patient's medical information in one sentence:\\n\\n{patient_info}\"}\n",
        "    ],\n",
        "    max_tokens=150\n",
        "    )\n",
        "    summary = response.choices[0].message.content\n",
        "    # print(type(summary))\n",
        "    print(summary)\n",
        "    return summary\n",
        "\n",
        "# Function to process all XML files in a directory\n",
        "def process_patient_directory(directory):\n",
        "    patient_summaries = []\n",
        "\n",
        "    # for filename in os.listdir(directory):\n",
        "    if directory.endswith(\".xml\"):\n",
        "        # xml_file = os.path.join(directory, filename)\n",
        "\n",
        "        # Extract patient info from XML\n",
        "        patient_id, patient_info = extract_patient_info(directory)\n",
        "\n",
        "        # Submit the extracted data to OpenAI and get the summary\n",
        "        summary = summarize_patient_info(patient_info)\n",
        "\n",
        "        # Add the summary to the dictionary\n",
        "        patient_summaries.append({\n",
        "            \"_id\": patient_id,\n",
        "            \"text\": summary\n",
        "        })\n",
        "\n",
        "    return patient_summaries"
      ],
      "metadata": {
        "id": "SRfKkx8P98FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3"
      ],
      "metadata": {
        "id": "um6DRn0AJK4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory where all XML files are stored\n",
        "# directory_path = 'D:\\Turmerik_3\\synthea_1m_fhir_3_0_May_24\\output_1\\CCDA'\n",
        "directory_path = '/content/Abbott509_Chase285_5.xml'\n",
        "\n",
        "    # Process the directory and get the patient summaries\n",
        "patient_summaries = process_patient_directory(directory_path)\n",
        "\n",
        "    # Output the final dictionary with all patient summaries\n",
        "print(patient_summaries)\n",
        "\n",
        "    # Optional: Save the patient summaries to a JSON file\n",
        "import json\n",
        "with open('queries.json', 'w') as f:\n",
        "  json.dump(patient_summaries, f, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_osAEfu-AGx",
        "outputId": "0f8048d1-5ed1-41b1-e735-222f69f30cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chase285 Abbott509, a White male born on February 2, 2012, with no known allergies, has a medical history of otitis media, streptococcal sore throat, viral sinusitis, and acute bronchitis, had multiple vaccinations including Hep B, DTaP, and influenza, and underwent respiratory therapy as part of his plan of care.\n",
            "[{'_id': '5925ebf53425de8bbf004f49', 'text': 'Chase285 Abbott509, a White male born on February 2, 2012, with no known allergies, has a medical history of otitis media, streptococcal sore throat, viral sinusitis, and acute bronchitis, had multiple vaccinations including Hep B, DTaP, and influenza, and underwent respiratory therapy as part of his plan of care.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TrialGPT Retrieval"
      ],
      "metadata": {
        "id": "CeH7cT8jNOl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4"
      ],
      "metadata": {
        "id": "_1hyx3koJMnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "generate the search keywords for each patient\n",
        "\"\"\"\n",
        "\n",
        "from openai import OpenAI\n",
        "import json\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "\t\t# api_key = \"Your API Key\"\n",
        "\t\tapi_key = 'Your API Keys'\n",
        ")\n",
        "\n",
        "\n",
        "def get_keyword_generation_messages(note):\n",
        "\tsystem = 'You are a helpful assistant and your task is to help search relevant clinical trials for a given patient description. Please first summarize the main medical problems of the patient. Then generate up to 32 key conditions for searching relevant clinical trials for this patient. The key condition list should be ranked by priority. Please output only a JSON dict formatted as Dict{{\"summary\": Str(summary), \"conditions\": List[Str(condition)]}}.'\n",
        "\n",
        "\tprompt =  f\"Here is the patient description: \\n{note}\\n\\nJSON output:\"\n",
        "\n",
        "\tmessages = [\n",
        "\t\t{\"role\": \"system\", \"content\": system},\n",
        "\t\t{\"role\": \"user\", \"content\": prompt}\n",
        "\t]\n",
        "\n",
        "\treturn messages"
      ],
      "metadata": {
        "id": "S69M5rjbx6hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5"
      ],
      "metadata": {
        "id": "uDveYitCJOhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = {}\n",
        "ret_trials = {}\n",
        "model = 'gpt-4o'\n",
        "\n",
        "with open(f\"/content/queries.jsonl\", \"r\") as f:\n",
        "  for line in f.readlines():\n",
        "    entry = json.loads(line)\n",
        "    messages = get_keyword_generation_messages(entry[\"text\"])\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "\t\t\tmodel=model,\n",
        "\t\t\tmessages=messages,\n",
        "      temperature=0,\n",
        "\t\t)\n",
        "\n",
        "    output = response.choices[0].message.content\n",
        "    output = output.strip(\"`\").strip(\"json\")\n",
        "\n",
        "    ret_trials[entry[\"_id\"]] = {}\n",
        "    ret_trials[entry[\"_id\"]][\"raw\"] = entry[\"text\"]\n",
        "    ret_trials[entry[\"_id\"]][\"gpt-4-turbo\"] = json.loads(output)\n",
        "\n",
        "    outputs[entry[\"_id\"]] = json.loads(output)\n",
        "\n",
        "    with open(f\"retrieval_keywords_{model}.json\", \"w\") as f:\n",
        "      json.dump(outputs, f, indent=4)\n",
        "\n",
        "    with open(f\"id2queries.json\", \"w\") as f:\n",
        "      json.dump(ret_trials, f, indent=4)"
      ],
      "metadata": {
        "id": "DiYwUhbbzRLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6"
      ],
      "metadata": {
        "id": "pPmUsXqQJP0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MZM548gB5E-",
        "outputId": "eda4a5f4-83f7-4c39-b97c-af3a879f8999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting beir\n",
            "  Downloading beir-2.0.0.tar.gz (53 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentence-transformers (from beir)\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pytrec_eval (from beir)\n",
            "  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss_cpu (from beir)\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting elasticsearch==7.9.1 (from beir)\n",
            "  Downloading elasticsearch-7.9.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting datasets (from beir)\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from elasticsearch==7.9.1->beir) (2.2.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elasticsearch==7.9.1->beir) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->beir)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (4.66.5)\n",
            "Collecting xxhash (from datasets->beir)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets->beir)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->beir) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (3.10.8)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->beir) (6.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->beir) (4.44.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->beir) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->beir) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->beir) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->beir) (10.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets->beir) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->beir) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->beir) (3.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers->beir) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers->beir) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers->beir) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers->beir) (0.19.1)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->beir) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->beir) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->beir) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->beir) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->beir) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->beir) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers->beir) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers->beir) (1.3.0)\n",
            "Downloading elasticsearch-7.9.1-py2.py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.2/219.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: beir, pytrec_eval\n",
            "  Building wheel for beir (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for beir: filename=beir-2.0.0-py3-none-any.whl size=63549 sha256=227bda7d77e5fce600ab6040f635f51ff7de0679f0989eedfcb5260db63c3099\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/14/96/c606ede3c10e9300ef771a6183af09d389459195ff5f854862\n",
            "  Building wheel for pytrec_eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytrec_eval: filename=pytrec_eval-0.5-cp310-cp310-linux_x86_64.whl size=308198 sha256=72d25a0ff792dadb652744d501bfe6d5c869fe9e88bffd0f7a2f8178170a42d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3a/cd/dcc1ddfc763987d5cb237165d8ac249aa98a23ab90f67317a8\n",
            "Successfully built beir pytrec_eval\n",
            "Installing collected packages: xxhash, pytrec_eval, faiss_cpu, elasticsearch, dill, multiprocess, sentence-transformers, datasets, beir\n",
            "Successfully installed beir-2.0.0 datasets-3.0.1 dill-0.3.8 elasticsearch-7.9.1 faiss_cpu-1.8.0.post1 multiprocess-0.70.16 pytrec_eval-0.5 sentence-transformers-3.1.1 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7"
      ],
      "metadata": {
        "id": "AUE4t9wxJQ1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank-bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kon1NNfbCJoV",
        "outputId": "06d845e8-b250-4868-beab-b7aa956b47e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank-bm25) (1.26.4)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8"
      ],
      "metadata": {
        "id": "E_ORaPH_JSnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "import faiss\n",
        "import json\n",
        "from nltk import word_tokenize\n",
        "import numpy as np\n",
        "import os\n",
        "from rank_bm25 import BM25Okapi\n",
        "import sys\n",
        "import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "metadata": {
        "id": "4TTFeg3B_uh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 9"
      ],
      "metadata": {
        "id": "u5OYitrFJTwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Conduct the first stage retrieval by the hybrid retriever\n",
        "\"\"\"\n",
        "\n",
        "def get_bm25_corpus_index(corpus):\n",
        "  corpus_path = os.path.join(f\"bm25_corpus_{corpus}.json\")\n",
        "\n",
        "\t# if already cached then load, otherwise build\n",
        "  if os.path.exists(corpus_path):\n",
        "    corpus_data = json.load(open(corpus_path))\n",
        "    tokenized_corpus = corpus_data[\"tokenized_corpus\"]\n",
        "    corpus_nctids = corpus_data[\"corpus_nctids\"]\n",
        "\n",
        "  else:\n",
        "    tokenized_corpus = []\n",
        "    corpus_nctids = []\n",
        "\n",
        "    with open(f\"corpus.jsonl\", \"r\") as f:\n",
        "      for line in f.readlines():\n",
        "        entry = json.loads(line)\n",
        "        corpus_nctids.append(entry[\"_id\"])\n",
        "\n",
        "        # weighting: 3 * title, 2 * condition, 1 * text\n",
        "        tokens = word_tokenize(entry[\"title\"].lower()) * 3\n",
        "        for disease in entry[\"metadata\"][\"diseases_list\"]:\n",
        "          tokens += word_tokenize(disease.lower()) * 2\n",
        "        tokens += word_tokenize(entry[\"text\"].lower())\n",
        "\n",
        "        tokenized_corpus.append(tokens)\n",
        "\n",
        "    corpus_data = {\n",
        "\t\t\t\"tokenized_corpus\": tokenized_corpus,\n",
        "\t\t\t\"corpus_nctids\": corpus_nctids,\n",
        "\t\t}\n",
        "\n",
        "    with open(corpus_path, \"w\") as f:\n",
        "      json.dump(corpus_data, f, indent=4)\n",
        "\n",
        "  bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "  return bm25, corpus_nctids\n",
        "\n",
        "\n",
        "def get_medcpt_corpus_index(corpus):\n",
        "  corpus_path = f\"{corpus}_embeds.npy\"\n",
        "  nctids_path = f\"{corpus}_nctids.json\"\n",
        "\n",
        "  if os.path.exists(corpus_path):\n",
        "    embeds = np.load(corpus_path)\n",
        "    corpus_nctids = json.load(open(nctids_path))\n",
        "\n",
        "  else:\n",
        "    embeds = []\n",
        "    corpus_nctids = []\n",
        "\n",
        "    model = AutoModel.from_pretrained(\"ncbi/MedCPT-Article-Encoder\").to(\"cuda\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"ncbi/MedCPT-Article-Encoder\")\n",
        "\n",
        "    with open(f\"corpus.jsonl\", \"r\") as f:\n",
        "      print(\"Encoding the corpus\")\n",
        "      for line in tqdm.tqdm(f.readlines()):\n",
        "        entry = json.loads(line)\n",
        "        corpus_nctids.append(entry[\"_id\"])\n",
        "\n",
        "        title = entry[\"title\"]\n",
        "        text = entry[\"text\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # tokenize the articles\n",
        "          encoded = tokenizer(\n",
        "              [[title, text]],\n",
        "              truncation=True,\n",
        "              padding=True,\n",
        "              return_tensors='pt',\n",
        "              max_length=512,\n",
        "          ).to(\"cuda\")\n",
        "\n",
        "          embed = model(**encoded).last_hidden_state[:, 0, :]\n",
        "\n",
        "          embeds.append(embed[0].cpu().numpy())\n",
        "\n",
        "    embeds = np.array(embeds)\n",
        "\n",
        "    np.save(corpus_path, embeds)\n",
        "    with open(nctids_path, \"w\") as f:\n",
        "      json.dump(corpus_nctids, f, indent=4)\n",
        "\n",
        "  index = faiss.IndexFlatIP(768)\n",
        "  index.add(embeds)\n",
        "\n",
        "  return index, corpus_nctids"
      ],
      "metadata": {
        "id": "u3omUw_m12eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 10"
      ],
      "metadata": {
        "id": "BzIJfdLlJZCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"Synthetic_Mass\"\n",
        "q_type = \"gpt-4-turbo\"\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# different k for fusion\n",
        "k = 20\n",
        "\n",
        "# bm25 weight\n",
        "bm25_wt = 1\n",
        "\n",
        "# medcpt weight\n",
        "medcpt_wt = 1\n",
        "\n",
        "# how many to rank\n",
        "N = 2000\n",
        "\n",
        "id2queries = json.load(open(f\"id2queries.json\"))\n",
        "\n",
        "trial_info = json.load(open(f\"trial_info.json\"))\n",
        "\n",
        "# loading the indices\n",
        "bm25, bm25_nctids = get_bm25_corpus_index(corpus)\n",
        "medcpt, medcpt_nctids = get_medcpt_corpus_index(corpus)\n",
        "\n",
        "# loading the query encoder for MedCPT\n",
        "model = AutoModel.from_pretrained(\"ncbi/MedCPT-Query-Encoder\").to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ncbi/MedCPT-Query-Encoder\")\n",
        "\n",
        "# then conduct the searches, saving top 1k\n",
        "output_path = f\"qid2nctids_results_{q_type}_{corpus}_k{k}_bm25wt{bm25_wt}_medcptwt{medcpt_wt}_N{N}.json\"\n",
        "\n",
        "qid2nctids = {}\n",
        "recalls = []\n",
        "\n",
        "retrieved_trials_final = []\n",
        "\n",
        "with open(f\"/content/queries.jsonl\", \"r\") as f:\n",
        "  for line in tqdm.tqdm(f.readlines()):\n",
        "    entry = json.loads(line)\n",
        "    query = entry[\"text\"]\n",
        "    qid = entry[\"_id\"]\n",
        "    print(qid)\n",
        "\n",
        "  if \"turbo\" in q_type:\n",
        "    conditions = id2queries[qid][q_type][\"conditions\"]\n",
        "\n",
        "  if len(conditions) == 0:\n",
        "    nctid2score = {}\n",
        "\n",
        "  else:\n",
        "    # a list of nctid lists for the bm25 retriever\n",
        "    bm25_condition_top_nctids = []\n",
        "\n",
        "    for condition in conditions:\n",
        "      tokens = word_tokenize(condition.lower())\n",
        "      top_nctids = bm25.get_top_n(tokens, bm25_nctids, n=N)\n",
        "      bm25_condition_top_nctids.append(top_nctids)\n",
        "\n",
        "    # doing MedCPT retrieval\n",
        "    with torch.no_grad():\n",
        "      encoded = tokenizer(\n",
        "          conditions,\n",
        "          truncation=True,\n",
        "          padding=True,\n",
        "          return_tensors='pt',\n",
        "          max_length=256,\n",
        "      ).to(\"cuda\")\n",
        "\n",
        "      # encode the queries (use the [CLS] last hidden states as the representations)\n",
        "      embeds = model(**encoded).last_hidden_state[:, 0, :].cpu().numpy()\n",
        "\n",
        "      # search the Faiss index\n",
        "      scores, inds = medcpt.search(embeds, k=N)\n",
        "\n",
        "    medcpt_condition_top_nctids = []\n",
        "    for ind_list in inds:\n",
        "      top_nctids = [medcpt_nctids[ind] for ind in ind_list]\n",
        "      medcpt_condition_top_nctids.append(top_nctids)\n",
        "\n",
        "    nctid2score = {}\n",
        "\n",
        "    for condition_idx, (bm25_top_nctids, medcpt_top_nctids) in enumerate(zip(bm25_condition_top_nctids, medcpt_condition_top_nctids)):\n",
        "      if bm25_wt > 0:\n",
        "        for rank, nctid in enumerate(bm25_top_nctids):\n",
        "          if nctid not in nctid2score:\n",
        "            nctid2score[nctid] = 0\n",
        "\n",
        "          nctid2score[nctid] += (1 / (rank + k)) * (1 / (condition_idx + 1))\n",
        "\n",
        "      if medcpt_wt > 0:\n",
        "        for rank, nctid in enumerate(medcpt_top_nctids):\n",
        "          if nctid not in nctid2score:\n",
        "            nctid2score[nctid] = 0\n",
        "\n",
        "          nctid2score[nctid] += (1 / (rank + k)) * (1 / (condition_idx + 1))\n",
        "\n",
        "  nctid2score = sorted(nctid2score.items(), key=lambda x: -x[1])\n",
        "  top_nctids = [nctid for nctid, _ in nctid2score[:N]]\n",
        "  qid2nctids[qid] = top_nctids\n",
        "\n",
        "  print(qid2nctids[qid])\n",
        "\n",
        "  retrieved_trials = {}\n",
        "  retrieved_trials[\"patient_id\"] = qid\n",
        "  retrieved_trials[\"patient\"] = query\n",
        "  retrieved_trials[\"trials\"] = []\n",
        "  for trial in qid2nctids[qid]:\n",
        "    retrieved_trials[\"trials\"].append(trial_info[trial])\n",
        "\n",
        "  retrieved_trials_final.append(retrieved_trials)\n",
        "\n",
        "with open(output_path, \"w\") as f:\n",
        "  json.dump(qid2nctids, f, indent=4)\n",
        "\n",
        "with open(\"retrieved_trials.json\", \"w\") as f:\n",
        "  json.dump(retrieved_trials_final, f, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3F5FEoX68fB",
        "outputId": "eae6b25a-20c7-41ec-823d-830259ec44c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding the corpus\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 29.52it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 9467.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigir-20141\n",
            "sigir-20142\n",
            "['NCT00665366', 'NCT00188279', 'NCT02110251', 'NCT02073188', 'NCT02490241']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trial Info"
      ],
      "metadata": {
        "id": "LtIDbX-zMuqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 11"
      ],
      "metadata": {
        "id": "gYFWk6gzJa1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the JSONL file\n",
        "input_file_path = '/content/corpus.jsonl'\n",
        "output_file_path = '/content/trial_info.json'\n",
        "\n",
        "# Prepare a dictionary for the output data\n",
        "trial_info = {}\n",
        "\n",
        "# Open and process the input file\n",
        "with open(input_file_path, 'r') as f:\n",
        "    for line in f:\n",
        "        # Load each line as a dictionary\n",
        "        entry = json.loads(line)\n",
        "\n",
        "        # Extract the _id and metadata\n",
        "        trial_id = entry[\"_id\"]\n",
        "        metadata = entry[\"metadata\"]\n",
        "\n",
        "        # Add the NCTID field in the metadata\n",
        "        metadata[\"NCTID\"] = trial_id\n",
        "\n",
        "        # Add this information to the trial_info dictionary\n",
        "        trial_info[trial_id] = metadata\n",
        "\n",
        "# Save the trial_info dictionary to a new JSON file\n",
        "with open(output_file_path, 'w') as out_file:\n",
        "    json.dump(trial_info, out_file, indent=4)\n",
        "\n",
        "print(f\"trial_info.json has been created at: {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dcty3FIMuLA",
        "outputId": "e68723f6-37cf-4e13-83e7-1efb434b9ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trial_info.json has been created at: /content/trial_info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TrialGPT Matching"
      ],
      "metadata": {
        "id": "LSg36dkEEy68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 12"
      ],
      "metadata": {
        "id": "DPo5eK_LJb7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import json\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import time\n",
        "import os\n",
        "\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cxxcqT9LVr-",
        "outputId": "8a9c1f86-b6ad-40f9-e84b-692052f2b55f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 13"
      ],
      "metadata": {
        "id": "p_D8883eJctg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TrialGPT-Matching main functions.\n",
        "\"\"\"\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key='Your API Keys',\n",
        ")\n",
        "\n",
        "def parse_criteria(criteria):\n",
        "\toutput = \"\"\n",
        "\tcriteria = criteria.split(\"\\n\\n\")\n",
        "\n",
        "\tidx = 0\n",
        "\tfor criterion in criteria:\n",
        "\t\tcriterion = criterion.strip()\n",
        "\n",
        "\t\tif \"inclusion criteria\" in criterion.lower() or \"exclusion criteria\" in criterion.lower():\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tif len(criterion) < 5:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\toutput += f\"{idx}. {criterion}\\n\"\n",
        "\t\tidx += 1\n",
        "\n",
        "\treturn output\n",
        "\n",
        "\n",
        "def print_trial(\n",
        "\ttrial_info: dict,\n",
        "\tinc_exc: str,\n",
        ") -> str:\n",
        "\t\"\"\"Given a dict of trial information, returns a string of trial.\"\"\"\n",
        "\n",
        "\ttrial = f\"Title: {trial_info['brief_title']}\\n\"\n",
        "\ttrial += f\"Target diseases: {', '.join(trial_info['diseases_list'])}\\n\"\n",
        "\ttrial += f\"Interventions: {', '.join(trial_info['drugs_list'])}\\n\"\n",
        "\ttrial += f\"Summary: {trial_info['brief_summary']}\\n\"\n",
        "\n",
        "\tif inc_exc == \"inclusion\":\n",
        "\t\ttrial += \"Inclusion criteria:\\n %s\\n\" % parse_criteria(trial_info['inclusion_criteria'])\n",
        "\telif inc_exc == \"exclusion\":\n",
        "\t\ttrial += \"Exclusion criteria:\\n %s\\n\" % parse_criteria(trial_info['exclusion_criteria'])\n",
        "\n",
        "\treturn trial\n",
        "\n",
        "\n",
        "def get_matching_prompt(\n",
        "\ttrial_info: dict,\n",
        "\tinc_exc: str,\n",
        "\tpatient: str,\n",
        ") -> str:\n",
        "\t\"\"\"Output the prompt.\"\"\"\n",
        "\tprompt = f\"You are a helpful assistant for clinical trial recruitment. Your task is to compare a given patient note and the {inc_exc} criteria of a clinical trial to determine the patient's eligibility at the criterion level.\\n\"\n",
        "\n",
        "\tif inc_exc == \"inclusion\":\n",
        "\t\tprompt += \"The factors that allow someone to participate in a clinical study are called inclusion criteria. They are based on characteristics such as age, gender, the type and stage of a disease, previous treatment history, and other medical conditions.\\n\"\n",
        "\n",
        "\telif inc_exc == \"exclusion\":\n",
        "\t\tprompt += \"The factors that disqualify someone from participating are called exclusion criteria. They are based on characteristics such as age, gender, the type and stage of a disease, previous treatment history, and other medical conditions.\\n\"\n",
        "\n",
        "\tprompt += f\"You should check the {inc_exc} criteria one-by-one, and output the following three elements for each criterion:\\n\"\n",
        "\tprompt += f\"\\tElement 1. For each {inc_exc} criterion, briefly generate your reasoning process: First, judge whether the criterion is not applicable (not very common), where the patient does not meet the premise of the criterion. Then, check if the patient note contains direct evidence. If so, judge whether the patient meets or does not meet the criterion. If there is no direct evidence, try to infer from existing evidence, and answer one question: If the criterion is true, is it possible that a good patient note will miss such information? If impossible, then you can assume that the criterion is not true. Otherwise, there is not enough information.\\n\"\n",
        "\tprompt += f\"\\tElement 2. If there is relevant information, you must generate a list of relevant sentence IDs in the patient note. If there is no relevant information, you must annotate an empty list.\\n\"\n",
        "\tprompt += f\"\\tElement 3. Classify the patient eligibility for this specific {inc_exc} criterion: \"\n",
        "\n",
        "\tif inc_exc == \"inclusion\":\n",
        "\t\tprompt += 'the label must be chosen from {\"not applicable\", \"not enough information\", \"included\", \"not included\"}. \"not applicable\" should only be used for criteria that are not applicable to the patient. \"not enough information\" should be used where the patient note does not contain sufficient information for making the classification. Try to use as less \"not enough information\" as possible because if the note does not mention a medically important fact, you can assume that the fact is not true for the patient. \"included\" denotes that the patient meets the inclusion criterion, while \"not included\" means the reverse.\\n'\n",
        "\telif inc_exc == \"exclusion\":\n",
        "\t\tprompt += 'the label must be chosen from {\"not applicable\", \"not enough information\", \"excluded\", \"not excluded\"}. \"not applicable\" should only be used for criteria that are not applicable to the patient. \"not enough information\" should be used where the patient note does not contain sufficient information for making the classification. Try to use as less \"not enough information\" as possible because if the note does not mention a medically important fact, you can assume that the fact is not true for the patient. \"excluded\" denotes that the patient meets the exclusion criterion and should be excluded in the trial, while \"not excluded\" means the reverse.\\n'\n",
        "\n",
        "\tprompt += \"You should output only a JSON dict exactly formatted as: dict{str(criterion_number): list[str(element_1_brief_reasoning), list[int(element_2_sentence_id)], str(element_3_eligibility_label)]}.\"\n",
        "\n",
        "\tuser_prompt = f\"Here is the patient note, each sentence is led by a sentence_id:\\n{patient}\\n\\n\"\n",
        "\tuser_prompt += f\"Here is the clinical trial:\\n{print_trial(trial_info, inc_exc)}\\n\\n\"\n",
        "\tuser_prompt += f\"Plain JSON output:\"\n",
        "\n",
        "\treturn prompt, user_prompt\n",
        "\n",
        "\n",
        "def trialgpt_matching(trial: dict, patient: str, model: str):\n",
        "\tresults = {}\n",
        "\n",
        "\t# doing inclusions and exclusions in separate prompts\n",
        "\tfor inc_exc in [\"inclusion\", \"exclusion\"]:\n",
        "\t\tsystem_prompt, user_prompt = get_matching_prompt(trial, inc_exc, patient)\n",
        "\n",
        "\t\tmessages = [\n",
        "\t\t\t{\"role\": \"system\", \"content\": system_prompt},\n",
        "\t\t\t{\"role\": \"user\", \"content\": user_prompt},\n",
        "\t\t]\n",
        "\n",
        "\t\tresponse = client.chat.completions.create(\n",
        "\t\t\tmodel=\"gpt-4o\",\n",
        "\t\t\tmessages=messages,\n",
        "\t\t\ttemperature=0,\n",
        "\t\t)\n",
        "\n",
        "\t\tmessage = response.choices[0].message.content.strip()\n",
        "\t\tmessage = message.strip(\"`\").strip(\"json\")\n",
        "\n",
        "\t\ttry:\n",
        "\t\t\tresults[inc_exc] = json.loads(message)\n",
        "\t\texcept:\n",
        "\t\t\tresults[inc_exc] = message\n",
        "\n",
        "\treturn results"
      ],
      "metadata": {
        "id": "Uj36sV_LE1Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 14"
      ],
      "metadata": {
        "id": "9FNziTkIJewx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Running the TrialGPT matching for three cohorts (sigir, TREC 2021, TREC 2022).\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import os\n",
        "import sys\n",
        "\n",
        "corpus = \"Synthetic_Mass\"\n",
        "model = \"gpt-4-turbo\"\n",
        "\n",
        "dataset = json.load(open(f\"retrieved_trials.json\"))\n",
        "\n",
        "output_path = f\"matching_results_{corpus}_{model}.json\"\n",
        "\n",
        "# Dict{Str(patient_id): Dict{Str(label): Dict{Str(trial_id): Str(output)}}}\n",
        "if os.path.exists(output_path):\n",
        "\toutput = json.load(open(output_path))\n",
        "else:\n",
        "\toutput = {}\n",
        "\n",
        "for instance in dataset:\n",
        "  # Dict{'patient': Str(patient), '0': Str(NCTID), ...}\n",
        "  patient_id = instance[\"patient_id\"]\n",
        "  patient = instance[\"patient\"]\n",
        "  sents = sent_tokenize(patient)\n",
        "  sents.append(\"The patient will provide informed consent, and will comply with the trial protocol without any practical issues.\")\n",
        "  sents = [f\"{idx}. {sent}\" for idx, sent in enumerate(sents)]\n",
        "  patient = \"\\n\".join(sents)\n",
        "\n",
        "  # initialize the patient id in the output\n",
        "  if patient_id not in output:\n",
        "    output[patient_id] = {\"trials\": {}}\n",
        "\n",
        "\t# for label in [\"2\", \"1\", \"0\"]:\n",
        "\t# \tif label not in instance: continue\n",
        "\n",
        "  for trial in instance[\"trials\"]:\n",
        "    trial_id = trial[\"NCTID\"]\n",
        "\n",
        "    # already calculated and cached\n",
        "    if trial_id in output[patient_id][\"trials\"]:\n",
        "      continue\n",
        "\n",
        "    # in case anything goes wrong (e.g., API calling errors)\n",
        "    try:\n",
        "      results = trialgpt_matching(trial, patient, model)\n",
        "      output[patient_id][\"trials\"][trial_id] = results\n",
        "\n",
        "      with open(output_path, \"w\") as f:\n",
        "        json.dump(output, f, indent=4)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      continue"
      ],
      "metadata": {
        "id": "7-gX6bozE4BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TrialGPT Ranking"
      ],
      "metadata": {
        "id": "VClOWxt9WBZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 15"
      ],
      "metadata": {
        "id": "lzWv32JCJgB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import json\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import time\n",
        "import os\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key='Your API Keys',\n",
        ")\n",
        "\n",
        "def convert_criteria_pred_to_string(prediction: dict,trial_info: dict,) -> str:\n",
        "  \"\"\"Given the TrialGPT prediction, output the linear string of the criteria.\"\"\"\n",
        "  output = \"\"\n",
        "\n",
        "  for inc_exc in [\"inclusion\", \"exclusion\"]:\n",
        "    # first get the idx2criterion dict\n",
        "    idx2criterion = {}\n",
        "    criteria = trial_info[inc_exc + \"_criteria\"].split(\"\\n\\n\")\n",
        "\n",
        "    idx = 0\n",
        "    for criterion in criteria:\n",
        "      criterion = criterion.strip()\n",
        "\n",
        "      if \"inclusion criteria\" in criterion.lower() or \"exclusion criteria\" in criterion.lower():\n",
        "        continue\n",
        "\n",
        "      if len(criterion) < 5:\n",
        "        continue\n",
        "\n",
        "      idx2criterion[str(idx)] = criterion\n",
        "      idx += 1\n",
        "\n",
        "    for idx, info in enumerate(prediction[inc_exc].items()):\n",
        "      criterion_idx, preds = info\n",
        "\n",
        "      if criterion_idx not in idx2criterion:\n",
        "        continue\n",
        "\n",
        "      criterion = idx2criterion[criterion_idx]\n",
        "\n",
        "      if len(preds) != 3:\n",
        "        continue\n",
        "\n",
        "      output += f\"{inc_exc} criterion {idx}: {criterion}\\n\"\n",
        "      output += f\"\\tPatient relevance: {preds[0]}\\n\"\n",
        "\n",
        "      if len(preds[1]) > 0:\n",
        "        output += f\"\\tEvident sentences: {preds[1]}\\n\"\n",
        "      output += f\"\\tPatient eligibility: {preds[2]}\\n\"\n",
        "\n",
        "  return output\n",
        "\n",
        "def convert_pred_to_prompt(patient: str,pred: dict,trial_info: dict,) -> str:\n",
        "  \"\"\"Convert the prediction to a prompt string.\"\"\"\n",
        "  # get the trial string\n",
        "  trial = f\"Title: {trial_info['brief_title']}\\n\"\n",
        "  trial += f\"Target conditions: {', '.join(trial_info['diseases_list'])}\\n\"\n",
        "  trial += f\"Summary: {trial_info['brief_summary']}\"\n",
        "\n",
        "  # then get the prediction strings\n",
        "  pred = convert_criteria_pred_to_string(pred, trial_info)\n",
        "\n",
        "  # construct the prompt\n",
        "  prompt = \"You are a helpful assistant for clinical trial recruitment. You will be given a patient note, a clinical trial, and the patient eligibility predictions for each criterion.\\n\"\n",
        "  prompt += \"Your task is to output two scores, a relevance score (R) and an eligibility score (E), between the patient and the clinical trial.\\n\"\n",
        "  prompt += \"First explain the consideration for determining patient-trial relevance. Predict the relevance score R (0~100), which represents the overall relevance between the patient and the clinical trial. R=0 denotes the patient is totally irrelevant to the clinical trial, and R=100 denotes the patient is exactly relevant to the clinical trial.\\n\"\n",
        "  prompt += \"Then explain the consideration for determining patient-trial eligibility. Predict the eligibility score E (-R~R), which represents the patient's eligibility to the clinical trial. Note that -R <= E <= R (the absolute value of eligibility cannot be higher than the relevance), where E=-R denotes that the patient is ineligible (not included by any inclusion criteria, or excluded by all exclusion criteria), E=R denotes that the patient is eligible (included by all inclusion criteria, and not excluded by any exclusion criteria), E=0 denotes the patient is neutral (i.e., no relevant information for all inclusion and exclusion criteria).\\n\"\n",
        "  prompt += 'Please output a JSON dict formatted as Dict{\"relevance_explanation\": Str, \"relevance_score_R\": Float, \"eligibility_explanation\": Str, \"eligibility_score_E\": Float, \"eligibilityCriteriaMet\": Str}.'\n",
        "  prompt += 'Make sure to mention just \"Yes\" or \"No\" for the \"eligibilityCriteriaMet\" key in the output JSON dict.'\n",
        "\n",
        "  user_prompt = \"Here is the patient note:\\n\"\n",
        "  user_prompt += patient + \"\\n\\n\"\n",
        "  user_prompt += \"Here is the clinical trial description:\\n\"\n",
        "  user_prompt += trial + \"\\n\\n\"\n",
        "  user_prompt += \"Here are the criterion-levle eligibility prediction:\\n\"\n",
        "  user_prompt += pred + \"\\n\\n\"\n",
        "  user_prompt += \"Plain JSON output:\"\n",
        "\n",
        "  return prompt, user_prompt\n",
        "\n",
        "\n",
        "def trialgpt_aggregation(patient: str, trial_results: dict, trial_info: dict, model: str):\n",
        "\tsystem_prompt, user_prompt = convert_pred_to_prompt(\n",
        "\t\t\tpatient,\n",
        "\t\t\ttrial_results,\n",
        "\t\t\ttrial_info\n",
        "\t)\n",
        "\n",
        "\tmessages = [\n",
        "\t\t{\"role\": \"system\", \"content\": system_prompt},\n",
        "\t\t{\"role\": \"user\", \"content\": user_prompt}\n",
        "\t]\n",
        "\n",
        "\tresponse = client.chat.completions.create(\n",
        "\t\tmodel=\"gpt-4o\",\n",
        "\t\tmessages=messages,\n",
        "\t\ttemperature=0,\n",
        "\t)\n",
        "\tresult = response.choices[0].message.content.strip()\n",
        "\tresult = result.strip(\"`\").strip(\"json\")\n",
        "\tresult = json.loads(result)\n",
        "\n",
        "\treturn result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLvTpgWsWDCI",
        "outputId": "47f4750c-c470-40d1-b257-a0f7e500436c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 16"
      ],
      "metadata": {
        "id": "7e873GkEJhGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Using GPT to aggregate the scores by itself.\n",
        "\"\"\"\n",
        "\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "import json\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import os\n",
        "import sys\n",
        "import time"
      ],
      "metadata": {
        "id": "cJUiX1lmXga9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 17"
      ],
      "metadata": {
        "id": "bYozZV70JkZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  corpus = \"Synthetic_Mass\"\n",
        "  model = \"gpt-4-turbo\"\n",
        "\n",
        "\t# the path of the matching results\n",
        "  matching_results_path = \"/content/matching_results_Synthetic_Mass_gpt-4-turbo.json\"\n",
        "  results = json.load(open(matching_results_path))\n",
        "\n",
        "  # loading the trial2info dict\n",
        "  trial2info = json.load(open(\"trial_info.json\"))\n",
        "\n",
        "  # loading the patient info\n",
        "  queries_path = \"/content/queries.jsonl\"\n",
        "  queries = {}\n",
        "  with open(queries_path, 'r') as f:\n",
        "    for line in f:\n",
        "      # Parse each line as JSON and append to the list\n",
        "      queries_dict = (json.loads(line))\n",
        "      queries[queries_dict[\"_id\"]] = queries_dict[\"text\"]\n",
        "\n",
        "  # output file path\n",
        "  output_path = f\"aggregation_results_{corpus}_{model}.json\"\n",
        "\n",
        "  if os.path.exists(output_path):\n",
        "    output = json.load(open(output_path))\n",
        "  else:\n",
        "    output = {}\n",
        "\n",
        "\t# patient-level\n",
        "  for patient_id, info in results.items():\n",
        "\t\t# get the patient note\n",
        "    patient = queries[patient_id]\n",
        "    sents = sent_tokenize(patient)\n",
        "    sents.append(\"The patient will provide informed consent, and will comply with the trial protocol without any practical issues.\")\n",
        "    sents = [f\"{idx}. {sent}\" for idx, sent in enumerate(sents)]\n",
        "    patient = \"\\n\".join(sents)\n",
        "\n",
        "    if patient_id not in output:\n",
        "      output[patient_id] = {}\n",
        "\n",
        "\t\t# label-level, 3 label / patient\n",
        "    for label, trials in info.items():\n",
        "\n",
        "\t\t\t# trial-level\n",
        "      for trial_id, trial_results in trials.items():\n",
        "\t\t\t\t# already cached results\n",
        "        if trial_id in output[patient_id]:\n",
        "          continue\n",
        "\n",
        "        if type(trial_results) is not dict:\n",
        "          output[patient_id][trial_id] = \"matching result error\"\n",
        "\n",
        "          with open(output_path, \"w\") as f:\n",
        "            json.dump(output, f, indent=4)\n",
        "\n",
        "          continue\n",
        "\n",
        "\t\t\t\t# specific trial information\n",
        "        trial_info = trial2info[trial_id]\n",
        "\n",
        "        try:\n",
        "          result = trialgpt_aggregation(patient, trial_results, trial_info, model)\n",
        "          output[patient_id][trial_id] = result\n",
        "\n",
        "          with open(output_path, \"w\") as f:\n",
        "            json.dump(output, f, indent=4)\n",
        "\n",
        "        except:\n",
        "          continue"
      ],
      "metadata": {
        "id": "TxN6gxguXrTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 18"
      ],
      "metadata": {
        "id": "8NRQnYiBJltR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Rank the trials given the matching and aggregation results\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import sys\n",
        "\n",
        "eps = 1e-9\n",
        "\n",
        "def get_matching_score(matching):\n",
        "\t# count only the valid ones\n",
        "\tincluded = 0\n",
        "\tnot_inc = 0\n",
        "\tna_inc = 0\n",
        "\tno_info_inc = 0\n",
        "\n",
        "\texcluded = 0\n",
        "\tnot_exc = 0\n",
        "\tna_exc = 0\n",
        "\tno_info_exc = 0\n",
        "\n",
        "\t# first count inclusions\n",
        "\tfor criteria, info in matching[\"inclusion\"].items():\n",
        "\n",
        "\t\tif len(info) != 3:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tif info[2] == \"included\":\n",
        "\t\t\tincluded += 1\n",
        "\t\telif info[2] == \"not included\":\n",
        "\t\t\tnot_inc += 1\n",
        "\t\telif info[2] == \"not applicable\":\n",
        "\t\t\tna_inc += 1\n",
        "\t\telif info[2] == \"not enough information\":\n",
        "\t\t\tno_info_inc += 1\n",
        "\n",
        "\t# then count exclusions\n",
        "\tfor criteria, info in matching[\"exclusion\"].items():\n",
        "\n",
        "\t\tif len(info) != 3:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tif info[2] == \"excluded\":\n",
        "\t\t\texcluded += 1\n",
        "\t\telif info[2] == \"not excluded\":\n",
        "\t\t\tnot_exc += 1\n",
        "\t\telif info[2] == \"not applicable\":\n",
        "\t\t\tna_exc += 1\n",
        "\t\telif info[2] == \"not enough information\":\n",
        "\t\t\tno_info_exc += 1\n",
        "\n",
        "\t# get the matching score\n",
        "\tscore = 0\n",
        "\n",
        "\tscore += included / (included + not_inc + no_info_inc + eps)\n",
        "\n",
        "\tif not_inc > 0:\n",
        "\t\tscore -= 1\n",
        "\n",
        "\tif excluded > 0:\n",
        "\t\tscore -= 1\n",
        "\n",
        "\treturn score\n",
        "\n",
        "\n",
        "def get_agg_score(assessment):\n",
        "\ttry:\n",
        "\t\trel_score = float(assessment[\"relevance_score_R\"])\n",
        "\t\teli_score = float(assessment[\"eligibility_score_E\"])\n",
        "\texcept:\n",
        "\t\trel_score = 0\n",
        "\t\teli_score = 0\n",
        "\n",
        "\tscore = (rel_score + eli_score) / 100\n",
        "\n",
        "\treturn score"
      ],
      "metadata": {
        "id": "6-ciNt6HgTk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 19"
      ],
      "metadata": {
        "id": "SFa9127PJnOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\t# args are the results paths\n",
        "  matching_results_path = \"/content/matching_results_Synthetic_Mass_gpt-4-turbo.json\"\n",
        "  agg_results_path = \"/content/aggregation_results_Synthetic_Mass_gpt-4-turbo.json\"\n",
        "  trial_info_path = \"/content/trial_info.json\"\n",
        "\n",
        "\t# loading the results\n",
        "  matching_results = json.load(open(matching_results_path))\n",
        "  agg_results = json.load(open(agg_results_path))\n",
        "  trial_info = json.load(open(trial_info_path))\n",
        "\n",
        "  final_result = []\n",
        "\n",
        "\t# loop over the patients\n",
        "  for patient_id, label2trial2results in matching_results.items():\n",
        "    trial2score = {}\n",
        "    result_dict = {}\n",
        "    result_dict[\"patientID\"] = patient_id\n",
        "\n",
        "    for _, trial2results in label2trial2results.items():\n",
        "      for trial_id, results in trial2results.items():\n",
        "        matching_score = get_matching_score(results)\n",
        "\n",
        "        if patient_id not in agg_results or trial_id not in agg_results[patient_id]:\n",
        "          print(f\"Patient {patient_id} Trial {trial_id} not in the aggregation results.\")\n",
        "          agg_score = 0\n",
        "        else:\n",
        "          agg_score = get_agg_score(agg_results[patient_id][trial_id])\n",
        "\n",
        "        trial_score = matching_score + agg_score\n",
        "\n",
        "        trial2score[trial_id] = trial_score\n",
        "\n",
        "    sorted_trial2score = sorted(trial2score.items(), key=lambda x: -x[1])\n",
        "\n",
        "    result_dict[\"eligibleTrials\"] = []\n",
        "\n",
        "    print()\n",
        "    print(f\"Patient ID: {patient_id}\")\n",
        "    print(\"Clinical trial ranking:\")\n",
        "\n",
        "    for trial, score in sorted_trial2score:\n",
        "      print(trial, score)\n",
        "      result_dict[\"eligibleTrials\"].append({\"trialID\": trial,\n",
        "                                            \"trialName\": trial_info[trial][\"brief_title\"],\n",
        "                                            \"score\":score,\n",
        "                                            \"eligibilityCriteriaMet\": agg_results[patient_id][trial][\"eligibilityCriteriaMet\"]})\n",
        "\n",
        "    final_result.append(result_dict)\n",
        "\n",
        "    print(\"===\")\n",
        "    print()\n",
        "\n",
        "  result_path = \"/content/eligibility_results.json\"\n",
        "  with open(result_path, \"w\") as f:\n",
        "    json.dump(final_result, f, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLECMcOSio1b",
        "outputId": "8ddc748b-1a39-4766-db63-dcf69df38e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Patient ID: sigir-20142\n",
            "Clinical trial ranking:\n",
            "NCT00665366 0.0\n",
            "NCT02110251 0.0\n",
            "NCT02490241 -0.80000000004\n",
            "NCT00188279 -1.0\n",
            "NCT02073188 -1.6666666667777779\n",
            "===\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 20"
      ],
      "metadata": {
        "id": "N-29SmYKJpUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7n5TSmelBod",
        "outputId": "bcb08489-0f3f-4762-fbfb-4190ff6ac1ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'patientID': 'sigir-20142',\n",
              "  'eligibleTrials': [{'trialID': 'NCT00665366',\n",
              "    'trialName': 'Study to Evaluate the Efficacy and Safety of Aripiprazole Administered With Lithium or Valproate Over 12 Weeks in the Treatment of Mania in Bipolar I Disorder',\n",
              "    'score': 0.0,\n",
              "    'eligibilityCriteriaMet': 'No'},\n",
              "   {'trialID': 'NCT02110251',\n",
              "    'trialName': 'Exercise Therapy With Risk Factor Management and Life Style Coaching After Vascular Intervention for Patients With Peripheral Arterial Disease',\n",
              "    'score': 0.0,\n",
              "    'eligibilityCriteriaMet': 'No'},\n",
              "   {'trialID': 'NCT02490241',\n",
              "    'trialName': 'Lithium Therapy: Understanding Mothers, Metabolism and Mood',\n",
              "    'score': -0.80000000004,\n",
              "    'eligibilityCriteriaMet': 'No'},\n",
              "   {'trialID': 'NCT00188279',\n",
              "    'trialName': 'Minimum Dose Computed Tomography of the Thorax for Follow-up in Patients With Resected Lung Carcinoma',\n",
              "    'score': -1.0,\n",
              "    'eligibilityCriteriaMet': 'No'},\n",
              "   {'trialID': 'NCT02073188',\n",
              "    'trialName': 'Comparative Study of a Smartphone-Linked Self-Monitoring System Versus a Traditional One for Improving Metabolic Control and Compliance to Self-Monitoring of Blood Glucose',\n",
              "    'score': -1.6666666667777779,\n",
              "    'eligibilityCriteriaMet': 'No'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    }
  ]
}